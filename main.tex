\documentclass[12pt]{article}

\input{Preamble.tex}

\usepackage{titlesec}

% for appendix environment
\usepackage[titletoc,toc,title,page]{appendix}

\newcommand{\lb}{\left(}
\newcommand{\rb}{\right)}

\newcommand{\mf}{\mathbf}

\usepackage{dsfont}

\newcommand{\bbS}{\mathds{S}}

\newcommand{\mL}{\mathcal{L}}
\newcommand{\intty}{\int\limits_{-\infty}^{+\infty}}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\titleformat{\section}{\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}{\normalfont\itshape\bfseries}{\thesubsection.}{0.5em}{}

\begin{document}

\section{Начальные распределения для задачи двух тел}

\subsection{MCMC-sampling}

Предположим мы генерируем последовательность случайных величин, $\left\{ X_0, X_1, X_2, \dots \right\}$, такую что в каждый момент $t \geq 0$ следующее состояние $X_{t + 1}$ выбирается исходя из распределения $P \lb X_{t+1} | X_t \rb$, которое зависит от текущего состояния $X_t$, но не от предыдущего набора состояний $\left\{ X_0, X_1, X_2 ... X_{t - 1} \right\}$. То есть, состояние $X_{t + 1}$ определяется исключительно предыдущим $X_t$. Такая последовательность состояний называется \textit{цепью Маркова}. \par
Рассмотрим алгоритм Метрополиса-Гастингса, позволяющий получать последовательность точек -- элементов Марковской цепи -- распределенную согласно заданной плотности вероятности $\pi(\cdot)$.
\begin{algorithm}
\begin{algorithmic}[1]
		\caption{Metropolis-Hastings algorithm [1]}\label{metropolis}
\State Initialize $x^{(0)} \sim q(x)$
\State \textbf{for} iteration $i = 1, 2, \dots$ \textbf{do}
\State \quad Propose: $x^{cand} \sim q \lb x^{(i)} | x^{(i-1)} \rb$
\State \quad Acceptance probability:
\State \qquad $\alpha \lb x^{cand} | x^{(i-1)} \rb = \min \left\{ 1, \frac{q \lb x^{(i-1)} | x^{cand} \rb \pi \lb x^{(cand)} \rb }{ q \lb x^{cand} | x^{(i-1)} \rb \pi \lb x^{(i-1)} \rb} \right\}$
\State \quad $u \sim$ Uniform(u; 0, 1)
\State \quad \textbf{if} $u < \alpha$ \textbf{then}
\State \qquad Accept the proposal: $x^{(i)} \gets x^{cand}$
\State \quad \textbf{else}
\State \qquad Reject the proposal: $X^{(i)} \gets x^{(i-1)}$
\State \quad \textbf{end if}
\State \textbf{end for}
\end{algorithmic}
\end{algorithm}

Первым шагом алгоритма является выбор случайной точки (эта величина выбирается определенным образом на основе распределения; я же выбирал ее совершенно случайным образом, но так, чтобы она не оказалась в какой-то физически маловероятной области). Следующий за ним главный цикл алгоритма состоит из трех частей: (1) Получать следующую точку ("кандидата") $x^{cand}$ исходя из вспомогательного распределения $q \lb x^{(i)} | x^{(i-1)} \rb$; (2) Рассчитать вероятность перехода в новую точку $\alpha \lb x^{cand} | x^{(i-1)} \rb$, основываясь на распределении $q$ и функции распределения $\pi$; (3) Принять новую точку с вероятностью $\alpha$. \par
Обратим внимание на то, что точка, полученная исходя из вспомогательного распределения $q(\cdot)$, принимается не всегда, а лишь с вероятностью $\alpha \lb \cdot \rb$. Рассматривают вспомогательные распределения двух классов -- симметричные и асимметричные. Симметричным называется распределение, удовлетворяющее следующему соотношению
\begin{gather}
		q \lb x^{(i)} | x^{(i-1)} \rb = q \lb x^{(i-1)} | x^{(i)} \rb \notag
\end{gather}
К часто используемым симметричным распределениям относятся гауссово и равномерное распределения. В качестве примера рассмотрим вспомогательное распределение Гауссса: 
\begin{gather}
		x^{cand} = x^{(i-1)} + Normal(0, \sigma) \notag
\end{gather}

Понятно, что $Normal( x^{cand} - x^{(i-1)}; 0, \sigma ) = Normal( x^{(i-1)} - x^{cand}; 0, \sigma)$, то есть Гауссово распределение в действительности задает симметричное вспомогательное распределение. Среднеквадратичное отклонение $\sigma$ является параметром модели. Значение этого параметра будет определять динамику Марковской цепи в рассматриваемом пространстве. \par
В случае симметричных вспомогательных распределений выражение для вероятности выбора новой точки $\alpha(\cdot)$ существенно упрощается:
\begin{gather}
		\alpha \lb x^{cand} | x^{(i-1)} \rb = \min \left\{ 1, \frac{\pi \lb x^{cand} \rb}{\pi \lb x^{(i - 1)} \rb} \right\} \notag 
\end{gather}

Заметим, что если плотность вероятности ( точнее говоря, величина, пропорциональная плотности вероятности ) в новой точке $\pi \lb x^{cand} \rb$ больше, чем плотность вероятности в текущей $\pi \lb x^{\lb i - 1 \rb} \rb$, то их отношение будет больше $1$, а значит вероятность перехода в новую точку будет равна 1: $\alpha \lb x^{cand} | x^{(i - 1)} \rb = 1$. Другими словами, если новая точка выбрана таким образом, что плотность вероятности в ней больше, чем в текущей, то в нее осуществляется переход. Устройство алгоритма таково, что Марковская цепь "склонна" посещать те точки пространства, в которых моделируемая плотность вероятности выше. Однако, если новая точка была выбрана таким образом, что плотность вероятности в ней меньше, чем в текущей, то тогда вероятность перейти в нее будет определяться отношением плотностей вероятности:
\begin{gather}
		\alpha \lb x^{cand} | x^{(i - 1)} \rb = \frac{\pi \lb x^{cand} \rb}{\pi \lb x^{(i - 1)} \rb} \notag
\end{gather}

То есть, если вероятность в новой точке будет мала по сравнению с текущей, то и переход в нее будет маловероятен. \par
Вид вероятности перехода в новую точку из текущей определяется \textit{условием детального баланса} [2]. Последнее гарантирует, что полученная Марковская цепь в действительности будет удовлетворять заданной плотности вероятности.

\subsection{Точные формулы для двухатомной системы}

Рассмотрим вектор, соединяющий центры атомов. Обозначим $\mf{r}$ его координаты в лабораторной системе координат, $\mf{R}$ -- в молекулярной системе координат. Производные $\mf{r}$ и $\mf{R}$ связаны при помощи матрицы эйлеровых углов $\bbS$ и угловой скорости $\mf{\Omega}$:
\begin{gather}
		\dot{\mf{r}} = \bbS^{-1} \lb \dot{\mf{R}} + \left[ \mf{\Omega} \times \mf{R} \right] \rb. \label{1} 
\end{gather}

Пусть атомы в молекулярной системе координат расположены на оси $Z$, в таком случае правая часть выражения \eqref{1} превращается в 
\begin{gather}
\dot{\mf{r}} = \bbS^{-1} \left\{
\begin{bmatrix}
		0 \\
		0 \\
		\dot{R}
\end{bmatrix}
+
\begin{bmatrix}
		\Omega_y R \\
		- \Omega_x R \\
		0
\end{bmatrix}
\right\} \notag \\
\bbS \dot{\mf{r}} =
\begin{bmatrix}
\Omega_y R \\
- \Omega_x R \\
\dot{R}
\end{bmatrix}. \label{2}
\end{gather}

Лагранжиан в молекулярной системе координат имеет следующий вид:
\begin{gather}
\mL = \frac{1}{2} \mu \dot{R}^2 + \frac{1}{2} \mf{\Omega}^\top
\begin{bmatrix}
		\mu R^2 & 0 & 0 \\
		0 & \mu R^2 & 0 \\
		0 & 0 & 0
\end{bmatrix}
\mf{\Omega} \notag
\end{gather}

Используя теорему Донкина, находим связь гамильтоновых переменных $\mf{J}$ и $\mf{p} = \left[ p_R \right]$ с лагранжевыми переменными $\mf{\Omega}$ и $\mf{q} = \left[ R \right]$:
\begin{gather}
\begin{aligned}
\mf{J} &= \frac{\partial \mL}{\partial \mf{\Omega}} = \bbI \mf{\Omega} \\
\mf{p} &= \frac{\partial \mL}{\partial \dot{\mf{q}}} = \bba \dot{\mf{q}}
\end{aligned}
\quad \implies \quad
\begin{aligned}
		J_x &= \mu R^2 \Omega_x \\
		J_y &= \mu R^2 \Omega_y \\
		p_R &= \mu \dot{R}
\end{aligned} \label{3}
\end{gather}

Выкладка в приложении \ref{app1} показывает, что каждая компонента $\dot{\mf{r}}$ имеет нормальное распределение $\dot{\mf{r}} \sim \mathcal{N} \lb 0, \displaystyle \frac{kT}{\mu} \rb$. Таким образом, 

\newpage
\section{Литература}
\begin{enumerate}
	\item Yildirim I. Bayesian Inference: Metropolis-Hastings Sampling. MIT Online Library
	\item Gilks, W.R., Richardson, S., \& Spiegelhalter, D.J. (1996). \textit{Markov Chain Monte Carlo in Practice}. London: Chapman and Hall.
\end{enumerate}

\newpage

\titleformat{\section}{\large\bfseries}{\appendixname~\thesection .}{0.5em}{}

\begin{appendices}
\section{Приложение А. Распределения в лабораторной системе координат} \label{app1}

Воспользуемся следующими двумя выводами из теории вероятностей:
\begin{enumerate}
\item Пусть случайная величина $\xi$ распределена с плотностью $f_\xi \left( x \right)$. Тогда случайная величина $\eta = a \xi + b$ распределена с плотностью
\begin{gather}
	f_\eta (x) = \frac{1}{|a|} f_\xi \lb \frac{x - b}{a} \rb \notag
\end{gather}
\item Если две \underline{независимые} случайные величины $X$ и $Y$ распределены с плотностями $X \sim f_1(x)$ и $Y \sim f_2(x)$ соответственно, то случайна величина $Z = X + Y$ распределена с плотностью
\begin{gather}
	g(z) = \int\limits_{-\infty}^{+\infty} f_1(x) f_2(z - x) dx \notag
\end{gather}
\end{enumerate}

Т.к. вектор $\mf{r} = \mf{r}_1 - \mf{r}_2$ равен разнице радиус-векторов двух атомов $\mf{r}_1$ и $\mf{r}_2$ в лабораторной системе координат соответственно, то $\dot{\mf{r}} = \dot{\mf{r}}_1 - \dot{\mf{r}}_2$. Используя п.1 и п.2 получим распределение для компонент $\mf{r}$:
\begin{gather}
\left\{
\begin{aligned}
		\dot{\mf{r}}_{1x} &\sim f_1(x) = \sqrt{ \frac{m_1}{2 \pi k T} } \exp \lb - \frac{m_1 x^2}{2 k T} \rb \\
		- \dot{\mf{r}}_{2x} &\sim f_2(x) = \sqrt{ \frac{m_2}{2 \pi k T} } \exp \lb - \frac{m_2 x^2}{2 k T} \rb 
\end{aligned} \notag \\
\dot{\mf{r}}_x \sim \int\limits_{-\infty}^{+\infty} f_1(x) f_2(z - x) dx = \frac{ \sqrt{m_1 m_2}}{2 \pi k T} \int\limits_{-\infty}^{+\infty} \exp \lb - \frac{m_1 x^2}{2 k T} \rb \exp \lb - \frac{m_2 \lb z - x \rb^2}{2 k T} \rb dx \label{r_distr}
\right.
\end{gather}

Отдельно рассмотрим получившийся интеграл:
\begin{gather}
		\intty \exp \lb - \frac{m_1 x^2}{2 k T} - \frac{m_2 \lb z - x \rb^2}{2 k T} \rb dx = \intty \exp \lb \frac{- \lb m_1 + m_2 \rb x^2 - m_2 z^2 + 2 m_2 z x}{2 k T} \rb dx = \notag \\
		= \intty \exp \lb - \frac{\lb \sqrt{m_1 + m_2} x - \frac{m_2}{\sqrt{m_1 + m_2}} z \rb^2}{2 k T} \rb \exp \lb - \frac{m_2 z^2 - \frac{m_2^2}{m_1 + m_2} z^2 }{2 k T} \rb dx = \notag \\
		= \left[ y = \frac{ \sqrt{m_1 + m_2} x - \frac{m_2}{\sqrt{m_1 + m_2}} z}{ \sqrt{2 k T} } \right] = \sqrt{ \frac{2 k T}{m_ 1 + m_2} } \exp \lb - \frac{m_1 m_2}{2 \lb m_1 + m_2 \rb k T} z^2 \rb \intty \exp \lb - y^2 \rb dy = \notag \\
		= \sqrt{ \frac{2 \pi k T}{m_1 + m_2} } \exp \lb - \frac{m_1 m_2}{2 \lb m_1 + m_2 \rb k T} z^2 \rb \label{int}
\end{gather}

Подставляя значение интеграла \eqref{int} в выражение для плотности распределения $\dot{\mf{r}}_x$ \eqref{r_distr}, получаем
\begin{gather}
		\dot{\mf{r}}_x \sim \frac{1}{\sqrt{2 \pi k T}} \sqrt{ \frac{m_1 m_2}{m_1 + m_2} } \exp \lb - \frac{m_1 m_2}{2 \lb m_1 + m_2 \rb k T} z^2 \rb = \sqrt{ \frac{\mu}{2 \pi k T} } \exp \lb - \frac{\mu z^2}{2 k T} \rb \notag, 
\end{gather}
где через $\mu$ была обозначена приведенная масса двухатомной системы $ \mu = \displaystyle \frac{m_1 m_2}{m_1 + m_2} $.


\end{appendices}


\end{document}
